# VizML - Lecture 9: Intrinsic Models and Interpretability

## Introduction
- Today's class focuses on **intrinsic models** that are intrinsically interpretable.
- Next class will cover **black box interpretability**.
- Intrinsic models allow understanding of how predictions are made due to their transparent nature.

## Linear Models
- **Linear Models** predict a variable using a set of features $x_1$ through $x_n$.
- Key characteristic: predictions are a weighted sum of features, allowing for interpretability.

### Interpretation of Linear Models
- Real numbers represent data features; linear models break predictions into a sum of feature vectors.
- Example: In a housing model, more bedrooms might increase the price by $19,000, reflecting real estate agent logic.
- Issues can arise with multicollinearity (e.g., number of rooms and square footage being related).

### Assumptions in Linear Models
- **Linearity**: Effects are additive, no interactions unless specified.
- **Independent Features**: Assumes features are not correlated.
- **Constant Error Variance**: No multicollinearity.
- **Variance Inflation Factor (VIF)**: Used to check for correlations; high VIF indicates problematic multicollinearity.

### Evaluation
- **R-squared Measure**: Closer to 1 indicates a better model fit.
- Example: Linear regression applied to house pricing estimates.

## Generalized Additive Models (GAMs)
- **GAMs** extend linear models by incorporating flexible functions $f_j(x_j)$ for each feature.
- GAMs use a **link function** $G$, such as a log, to model distribution functions.
- **Partial Dependency Plots (PDPs)**: Show how the model prediction depends on a single feature while averaging out others.

### Advantages and Challenges
- GAMs can capture non-linear relationships.
- Computationally expensive to compute.
- More interpretable than black-box models but less intuitive than simple linear coefficients.

## Explainable Boosting Machines (EBMs)
- An advanced form of GAMs.
- Achieve high accuracy and interpretability by using small trees for each feature in a boosting process.

### Training EBMs
- Train small trees on each feature, update residuals, and repeat for multiple iterations.
- Summarize the trees into graphs to visualize feature impact.

## Decision Trees
- **Decision Trees**: Visual representation of decisions and their possible consequences.
- Interpretable but can become complex as they grow in size.

### Random Forests and Visualization
- **Random Forests**: Use multiple decision trees to improve prediction accuracy.
- Visualization tools help interpret the ensemble of trees.

## Decision Rules
- **Decision Rules**: Simple if-then statements derived from decision trees.
- Multiple rules can apply simultaneously, requiring a method to resolve conflicts.

## Surrogate Models
- Used to approximate complex models with simpler, interpretable versions.
- Balances fidelity and interpretability.

## Summary
- Intrinsic models like linear models, GAMs, and decision trees offer different levels of interpretability and complexity.
- Tools like EBMs and visualization software enhance model understanding.
- Important to understand the trade-offs between model complexity, accuracy, and interpretability.

## Action Items
- Read recommended papers for deeper understanding.
- Participate in the lab session with Pari to apply concepts.
- Review visualization tools and techniques discussed for practical insights.

## Closing Remarks
- The lecture emphasized the importance of interpretable models in machine learning.
- Students are encouraged to explore the provided resources and apply the concepts in practical scenarios.